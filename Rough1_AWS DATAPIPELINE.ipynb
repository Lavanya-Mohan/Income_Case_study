{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a2d14f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "                               ####   IMPORT LIBRARIES   ###\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import avg, round, col,sum,count,split,weekofyear,struct,regexp_extract\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import from_unixtime,to_date, unix_timestamp\n",
    "\n",
    "category_mapping = {\n",
    "\"1\": \"Smartphone\",\n",
    "\"2\": \"Charging Cable\",\n",
    "\"3\": \"Headphones\",\n",
    "\"4\": \"Monitor\",\n",
    "\"5\": \"Batteries\",\n",
    "\"6\": \"Laptop\",\n",
    "\"7\": \"TV\",\n",
    "\"8\": \"Dryer\",\n",
    "\"9\": \"Washing Machine\",\n",
    "\"10\": \"others\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "replacements = {\n",
    "    \"int\": 0,\n",
    "    \"double\": 0.0,\n",
    "    \"float\": 0.0,\n",
    "    \"string\": \"\"\n",
    "}\n",
    "\n",
    "def create_spark_session(app_name: str) -> SparkSession:\n",
    "    \"\"\" Create a spark session.\n",
    "    \"\"\"\n",
    "    ss = SparkSession.builder.appName(app_name).config(\"spark.jars\", \"mysql-connector-j-8.3.0.jar\").getOrCreate()#.master('local').appName(app_name).getOrCreate()\n",
    "    return ss\n",
    "\n",
    "\n",
    "def read_in_data(sc: SparkSession, file: str):\n",
    "    \"\"\" Return a spark DataFrame of the excel file <file>.\n",
    "    \"\"\"\n",
    "    return sc.read.csv(file, header='true', sep=',', inferSchema=True)\n",
    "\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    \"\"\" Return a spark DataFrame combining all the dfs.\n",
    "    \"\"\"\n",
    "    return reduce(DataFrame.unionAll, dfs) \n",
    "\n",
    "def missing_values_imputation(df):\n",
    "    \"\"\" Return a spark DataFrame with missing values computes.\n",
    "    \"\"\"\n",
    "    for col_name, data_type in df.dtypes:\n",
    "            replacement_value = replacements.get(data_type.lower(), None)\n",
    "            if replacement_value is not None:\n",
    "                df = df.withColumn(col_name, when(col(col_name).isNull(), replacement_value).otherwise(col(col_name)))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def extract_date_columns_liquor(df,date_col):\n",
    "    \"\"\" Return a spark DataFrame with derived columns from date in liquor.\n",
    "    \"\"\"\n",
    "    df=df.withColumn('year', split(liquor_sales_df[date_col], '/').getItem(2)) \\\n",
    "   .withColumn('month', split(liquor_sales_df[date_col], '/').getItem(0)) \n",
    "    df=df.withColumn(date_col, to_date(date_col, \"MM/dd/yyyy\"))\n",
    "    df=df.withColumn(\"week\", weekofyear(date_col))\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_lat_long(df,point_col):\n",
    "    \"\"\" Return a spark DataFrame with derived columns lat and long from point.\n",
    "    \"\"\"\n",
    "    df=df.withColumn(\"geometry_cleaned\", regexp_replace(point_col, \"POINT \\\\(\", \"\"))\\\n",
    "    .withColumn(\"geometry_cleaned\", regexp_replace(\"geometry_cleaned\", \"\\\\)\", \"\"))\\\n",
    "    .withColumn(\"lat\", split(col(\"geometry_cleaned\"), \" \")[1].cast(\"float\")) \\\n",
    "    .withColumn(\"lon\", split(col(\"geometry_cleaned\"), \" \")[0].cast(\"float\"))\\\n",
    "    .drop(\"geometry_cleaned\")\n",
    "    return df  \n",
    "\n",
    "def extract_date_columns_electronics(df,date_col):\n",
    "    \"\"\" Return a spark DataFrame with derived columns from date in electronics.\n",
    "    \"\"\"\n",
    "    df=df.withColumn('year', split(df[date_col], '-').getItem(0)) \\\n",
    "   .withColumn('month', split(df[date_col], '-').getItem(1)) \n",
    "    df = df.withColumn(\"orderDateOnly\", to_date(date_col))\n",
    "    df=df.withColumn(\"week\", weekofyear(date_col))\n",
    "    return df\n",
    "\n",
    "def split_address(df,address_col):\n",
    "    \"\"\" Return a spark DataFrame with derived column like city,state from address.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"city\", split(df[address_col],\",\").getItem(1))\\\n",
    "    .withColumn(\"state_code\", split(df[address_col],\",\").getItem(2))\n",
    "    df=df.withColumn(\"state\", split(df[\"state_code\"], \" \").getItem(1))\n",
    "    df = df.drop(\"state_code\")\n",
    "    return df                                        \n",
    "\n",
    "def write_to_rds_mysql_db(df,table):\n",
    "    \"\"\" Loads data into RDS.\n",
    "    \"\"\"               \n",
    "    SERVER_ADDR_port = \"income-mysql-db.cjuyq4k026ji.us-east-2.rds.amazonaws.com:3306\"\n",
    "    db_name = \"income\" \n",
    "    table_name = \"electronicsDataMart\"  \n",
    "    df.limit(10).write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"url\", f\"jdbc:mysql://{SERVER_ADDR_port}/{db_name}\") \\\n",
    "        .option(\"dbtable\", table) \\\n",
    "        .option(\"user\", \"admin\") \\\n",
    "        .option(\"password\", \"Logindatabase24\") \\\n",
    "        .save()\n",
    "    return \"Loaded into the database\"   \n",
    "                                        \n",
    "                                        \n",
    "                                        \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # create a spark session\n",
    "    spark = create_spark_session(\"datapipeline\")\n",
    "\n",
    "    #### EXTRACT ####\n",
    "    ####  READ BOOKS DATA ####\n",
    "    \n",
    "    # read in the books_data.csv as a spark DataFrame\n",
    "    books_df = read_in_data(spark, 'books_data.csv')\n",
    "\n",
    "    # read in the Books_ratingas a spark DataFrame\n",
    "    books_ratings_df = read_in_data(spark, 'Books_rating.csv')\n",
    "    \n",
    "    ####  READ SALES DATA ####\n",
    "    # read in the Sales_2019.csv's a spark DataFrame\n",
    "    sales_jan_df= read_in_data(spark, 'Sales_January_2019.csv')\n",
    "    sales_feb_df= read_in_data(spark, 'Sales_February_2019.csv')\n",
    "    sales_mar_df= read_in_data(spark, 'Sales_March_2019.csv')\n",
    "    sales_april_df= read_in_data(spark, 'Sales_April_2019.csv')\n",
    "    sales_may_df= read_in_data(spark, 'Sales_May_2019.csv')\n",
    "    sales_june_df= read_in_data(spark, 'Sales_June_2019.csv')\n",
    "    sales_july_df= read_in_data(spark, 'Sales_July_2019.csv')\n",
    "    sales_aug_df= read_in_data(spark, 'Sales_August_2019.csv')\n",
    "    sales_sep_df= read_in_data(spark, 'Sales_September_2019.csv')\n",
    "    sales_oct_df= read_in_data(spark, 'Sales_October_2019.csv')\n",
    "    sales_nov_df= read_in_data(spark, 'Sales_November_2019.csv')\n",
    "    sales_dec_df= read_in_data(spark, 'Sales_December_2019.csv')\n",
    "    \n",
    "    \n",
    "    ####  LIQUOR SALES DATA ####\n",
    "    # read in the liquor.csv's a spark DataFrame\n",
    "    liquor_sales_df= read_in_data(spark, 'Liquor_Sales.csv')\n",
    "    \n",
    "    #### TRANSFORM LIQUOR ####\n",
    "    liquor_date_df=extract_date_columns_liquor(liquor_sales_df,\"Date\")\n",
    "    liquor_date_df= get_lat_long(liquor_date_df,\"Store Location\")    \n",
    "    liquor_data_mart_df=missing_values_imputation(liquor_date_df)\n",
    "\n",
    "    #### TRANSFORM BOOKS  ####\n",
    "    books_ratings_df=books_ratings_df.withColumn('Price',col('Price').cast(\"integer\"))\n",
    "    books_ratings_df = books_ratings_df.withColumn(\"review/time\", from_unixtime(\"review/time\"))\n",
    "    books_df_joined=books_df.join(books_ratings_df,books_df.Title ==  books_ratings_df.Title,\"inner\").drop(books_ratings_df.Title)\n",
    "    books_data_mart=missing_values_imputation(books_df_joined)\n",
    "\n",
    "        #### NLP IMPUTATIONS ####\n",
    "    replacement_value = \"\"\n",
    "\n",
    "    col_onlystr_nourl_nonum = ['Title', 'description','authors','publisher','categories','User_id',]\n",
    "    for col_name in col_onlystr_nourl_nonum:\n",
    "        # Filter out full number values and URL links -- only string no numbers no urls\n",
    "        books_data_mart = books_data_mart.withColumn(col_name, when(col(col_name).isNull() | (col(col_name) == \"\") | (regexp_extract(col(col_name), r'^\\d+$', 0) != \"\") | (regexp_extract(col(col_name), r'^((http|https|ftp):\\/\\/[^\\s\\/$.?#].[^\\s]*)$', 0) != \"\"), replacement_value).otherwise(col(col_name)))\n",
    "\n",
    "    col_url = ['image', 'previewLink', 'infoLink',]\n",
    "    for col_name in col_url:\n",
    "        # Filter out non-URL values -- only url link\n",
    "        books_data_mart = books_data_mart.withColumn(col_name, when(col(col_name).isNull() | (col(col_name) == \"\") | (regexp_extract(col(col_name), r'^((http|https|ftp):\\/\\/[^\\s\\/$.?#].[^\\s]*)$', 0) == \"\"), replacement_value).otherwise(col(col_name)))\n",
    "\n",
    "    col_onlyfloat_num = ['publishedDate','ratingsCount','review/score','review/time', 'Price']\n",
    "    for col_name in col_onlyfloat_num:\n",
    "        # Filter out non-numeric string values and URL links -- only float , number \n",
    "        books_data_mart = books_data_mart.withColumn(col_name, when(col(col_name).isNull() | (col(col_name) == \"\") | (regexp_extract(col(col_name), r'^\\d+(\\.\\d+)?$', 0) == \"\") | (regexp_extract(col(col_name), r'^((http|https|ftp):\\/\\/[^\\s\\/$.?#].[^\\s]*)$', 0) != \"\"), replacement_value).otherwise(col(col_name)))\n",
    "\n",
    "    col_onlyfloat_num_slash = ['review/helpfulness',]\n",
    "    for col_name in col_onlyfloat_num_slash:\n",
    "        # Filter out non-numeric string values and URL links\n",
    "        books_data_mart = books_data_mart.withColumn(col_name, when(col(col_name).isNull() | (col(col_name) == \"\") | (regexp_extract(col(col_name), r'^\\d+(\\.\\d+)?(/\\d+(\\.\\d+)?)?$', 0) == \"\") | (regexp_extract(col(col_name), r'^((http|https|ftp):\\/\\/[^\\s\\/$.?#].[^\\s]*)$', 0) != \"\"), replacement_value).otherwise(col(col_name)))\n",
    "\n",
    "        \n",
    "    #### TRANSFORM ELECTRONICS  ####\n",
    "    df_sales=unionAll(*[sales_jan_df, sales_feb_df,sales_mar_df,sales_april_df,sales_may_df,sales_june_df\n",
    "           ,sales_july_df,sales_aug_df,sales_sep_df,sales_oct_df,sales_nov_df,sales_dec_df])\n",
    "    \n",
    "    ### SPLIT ADDRESS ###\n",
    "    df_sales=df_sales.withColumn(\"Order Date\", to_date(unix_timestamp(\"Order Date\", \"MM/dd/yy HH:mm\").cast(\"timestamp\")))\n",
    "    df_sales_split=split_address(df_sales,\"Purchase Address\")\n",
    "    \n",
    "    ### date operations ###\n",
    "    df_date=extract_date_columns_electronics(df_sales_split,\"Order Date\")\n",
    "    df_date=df_date.withColumn(\"category\",\n",
    "        when(col(\"product\").contains(\"Batteries\"), \"Batteries\")\n",
    "       .when(col(\"product\").contains(\"Charging Cable\"), \"Charging Cable\")\n",
    "       .when(col(\"product\").contains(\"Phone\"), \"Smartphone\")\n",
    "       .when(col(\"product\").contains(\"Headphones\"), \"Headphones\")\n",
    "       .when(col(\"product\").contains(\"Laptop\"), \"Laptop\")\n",
    "       .when(col(\"product\").contains(\"TV\"), \"TV\")\n",
    "       .when(col(\"product\").contains(\"Dryer\"), \"Dryer\")\n",
    "       .when(col(\"product\").contains(\"Washing Machine\"), \"Washing Machine\")\n",
    "       .otherwise(\"Others\"))\n",
    "    electronics_data_mart=missing_values_imputation(df_date)\n",
    "  \n",
    "\n",
    "           ####  LOADING INTO DATABASE  ####                                \n",
    "\n",
    "#### Electronics data mart ####                                        \n",
    "                                        \n",
    "#write_to_rds_mysql_db(electronics_data_mart,\"electronicsDataMart\")  \n",
    "                                        \n",
    "#### Liquor data mart  ####                                   \n",
    "#write_to_rds_mysql_db(liquor_data_mart_df,\"liquorDataMart\")  \n",
    "                                                                                \n",
    "#### books data mart  ####                                             \n",
    "#write_to_rds_mysql_db(books_data_mart,\"booksDataMart\") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa68b1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------------+----------+----------+--------------------+-------+-----+----+-----+-------------+----+----------+\n",
      "|Order ID|Product|Quantity Ordered|Price Each|Order Date|    Purchase Address|   city|state|year|month|orderDateOnly|week|  category|\n",
      "+--------+-------+----------------+----------+----------+--------------------+-------+-----+----+-----+-------------+----+----------+\n",
      "|  141234| iPhone|               1|     700.0|2019-01-22|944 Walnut St, Bo...| Boston|   MA|2019|   01|   2019-01-22|   4|Smartphone|\n",
      "+--------+-------+----------------+----------+----------+--------------------+-------+-----+----+-----+-------------+----+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "electronics_data_mart.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06f71771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-----------+---------+--------------------+--------+----------+------------+----------+-----+-------------+-------------------+--------------------+--------------------+-----------+--------------------+--------------------+\n",
      "|               Title|         description|             authors|               image|previewLink|publisher|       publishedDate|infoLink|categories|ratingsCount|        Id|Price|      User_id|        profileName|  review/helpfulness|        review/score|review/time|      review/summary|         review/text|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----------+---------+--------------------+--------+----------+------------+----------+-----+-------------+-------------------+--------------------+--------------------+-----------+--------------------+--------------------+\n",
      "|\"\"\"Always ready!\"...|\",,['Kensil Bell'...|&dq=%22Always+rea...|&hl=&cd=1&source=...|           |     1943|http://books.goog...|        |          |            |B0007H2HAM|    0| stout hearts| and alert minds\"\"\"|\"It's hard to bel...| it's the narrati...|           | Bell had already...| 1941) which was ...|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----------+---------+--------------------+--------+----------+------------+----------+-----+-------------+-------------------+--------------------+--------------------+-----------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_data_mart.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d98b017f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+------------+---------------+---------------+----------+--------+--------------------+-------------+------+--------+--------------+-------------+--------------------+-----------+--------------------+----+------------------+-----------------+-------------------+------------+--------------+--------------------+---------------------+----+-----+----+-----------------+------------------+\n",
      "|Invoice/Item Number|      Date|Store Number|     Store Name|        Address|      City|Zip Code|      Store Location|County Number|County|Category| Category Name|Vendor Number|         Vendor Name|Item Number|    Item Description|Pack|Bottle Volume (ml)|State Bottle Cost|State Bottle Retail|Bottles Sold|Sale (Dollars)|Volume Sold (Liters)|Volume Sold (Gallons)|year|month|week|              lat|               lon|\n",
      "+-------------------+----------+------------+---------------+---------------+----------+--------+--------------------+-------------+------+--------+--------------+-------------+--------------------+-----------+--------------------+----+------------------+-----------------+-------------------+------------+--------------+--------------------+---------------------+----+-----+----+-----------------+------------------+\n",
      "|       S24127700024|2015-02-19|        3678|Smoke Shop, The|1918 SE 14TH ST|DES MOINES|   50320|POINT (-93.597011...|           77|  Polk| 1031200|VODKA FLAVORED|          380|Phillips Beverage...|      41783|Uv Blue Raspberry...|   6|               500|             4.89|               7.34|           2|         14.68|                 1.0|                 0.26|2015|   02|   8|41.57084274291992|-93.59700775146484|\n",
      "+-------------------+----------+------------+---------------+---------------+----------+--------+--------------------+-------------+------+--------+--------------+-------------+--------------------+-----------+--------------------+----+------------------+-----------------+-------------------+------------+--------------+--------------------+---------------------+----+-----+----+-----------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "liquor_data_mart_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dd4f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064b56c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c872b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7059a662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194ee87b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e6bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63352f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb414247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d50bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a2eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a64a94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
