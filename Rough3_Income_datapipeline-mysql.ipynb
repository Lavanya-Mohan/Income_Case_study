{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8c8cd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\vigne_u98gpxi\\\\Desktop\\\\Lavviggy\\\\Lav\\\\HuntJob\\\\Income\\\\DE_CaseStudy_Dataset\\\\Sales_Data'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520d91da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n",
      "C:\\Users\\vigne_u98gpxi\\anaconda3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(sys.exec_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4481b4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "43c67853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, round, col,sum,count,split,weekofyear,struct,regexp_extract,udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "674771d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import avg, round, col,sum,count,split,weekofyear,struct,regexp_extract\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import from_unixtime,to_date, unix_timestamp\n",
    "#import org.apache.spark.sql.functions.col\n",
    "#import org.apache.spark.sql.types.IntegerType\n",
    "\n",
    "def create_spark_session(app_name: str) -> SparkSession:\n",
    "    \"\"\" Create a spark session.\n",
    "    \"\"\"\n",
    "    ss = SparkSession.builder.appName(app_name).config(\"spark.jars\", \"mysql-connector-j-8.3.0.jar\").getOrCreate()#.master('local').appName(app_name).getOrCreate()\n",
    "    return ss\n",
    "\n",
    "\n",
    "def read_in_data(sc: SparkSession, file: str):\n",
    "    \"\"\" Return a spark DataFrame of the excel file <file>.\n",
    "    \"\"\"\n",
    "    return sc.read.csv(file, header='true', sep=',', inferSchema=True)\n",
    "\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    \"\"\" Return a spark DataFrame combining all the dfs.\n",
    "    \"\"\"\n",
    "    return reduce(DataFrame.unionAll, dfs) \n",
    "\n",
    "\n",
    "replacements = {\n",
    "    \"int\": 0,\n",
    "    \"double\": 0.0,\n",
    "    \"float\": 0.0,\n",
    "    \"string\": \"\"\n",
    "}\n",
    "\n",
    "def missing_values_imputation(df):\n",
    "    \"\"\" Return a spark DataFrame with missing values computes.\n",
    "    \"\"\"\n",
    "    for col_name, data_type in df.dtypes:\n",
    "            replacement_value = replacements.get(data_type.lower(), None)\n",
    "            if replacement_value is not None:\n",
    "                df = df.withColumn(col_name, when(col(col_name).isNull(), replacement_value).otherwise(col(col_name)))\n",
    "    return df\n",
    "    \n",
    "\n",
    "       \n",
    "\n",
    "# # def output_result(df: DataFrame, output_location: str, output_folder: str) -> None:\n",
    "# #     \"\"\" Save the DataFrame <df> as a csv file in the location specified by\n",
    "# #     <output_location>.\n",
    "# #     \"\"\"\n",
    "# #     df.coalesce(1).write.csv(path=output_location + output_folder,\n",
    "#                              mode='append', header=True)\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # create a spark session\n",
    "    spark = create_spark_session(\"datapipeline\")\n",
    "\n",
    "    #### EXTRACT ####\n",
    "    \n",
    "    ####  READ BOOKS DATA ####\n",
    "    \n",
    "    # read in the books_data.csv as a spark DataFrame\n",
    "    books_df = read_in_data(spark, 'books_data.csv')\n",
    "\n",
    "    # read in the Books_ratingas a spark DataFrame\n",
    "    books_ratings_df = read_in_data(spark, 'Books_rating.csv')\n",
    "    \n",
    "    \n",
    "    ####  READ SALES DATA ####\n",
    "    # read in the Sales_2019.csv's a spark DataFrame\n",
    "    \n",
    "    sales_jan_df= read_in_data(spark, 'Sales_January_2019.csv')\n",
    "    sales_feb_df= read_in_data(spark, 'Sales_February_2019.csv')\n",
    "    sales_mar_df= read_in_data(spark, 'Sales_March_2019.csv')\n",
    "    sales_april_df= read_in_data(spark, 'Sales_April_2019.csv')\n",
    "    sales_may_df= read_in_data(spark, 'Sales_May_2019.csv')\n",
    "    sales_june_df= read_in_data(spark, 'Sales_June_2019.csv')\n",
    "    sales_july_df= read_in_data(spark, 'Sales_July_2019.csv')\n",
    "    sales_aug_df= read_in_data(spark, 'Sales_August_2019.csv')\n",
    "    sales_sep_df= read_in_data(spark, 'Sales_September_2019.csv')\n",
    "    sales_oct_df= read_in_data(spark, 'Sales_October_2019.csv')\n",
    "    sales_nov_df= read_in_data(spark, 'Sales_November_2019.csv')\n",
    "    sales_dec_df= read_in_data(spark, 'Sales_December_2019.csv')\n",
    "    \n",
    "    \n",
    "    ####  LIQUOR SALES DATA ####\n",
    "    # read in the liquor.csv's a spark DataFrame\n",
    "    \n",
    "    liquor_sales_df= read_in_data(spark, 'Liquor_Sales.csv')\n",
    "    \n",
    "    #### TRANSFORM ####\n",
    "    def extract_date_columns_liquor(df,date_col):\n",
    "        df=df.withColumn('year', split(liquor_sales_df[date_col], '/').getItem(2)) \\\n",
    "       .withColumn('month', split(liquor_sales_df[date_col], '/').getItem(0)) \n",
    "        df=df.withColumn(date_col, to_date(date_col, \"MM/dd/yyyy\"))\n",
    "        df=df.withColumn(\"week\", weekofyear(date_col))\n",
    "        return df\n",
    "        \n",
    "        \n",
    "#     liquor_date_df=liquor_sales_df.withColumn('year', split(liquor_sales_df['Date'], '/').getItem(2)) \\\n",
    "#    .withColumn('month', split(liquor_sales_df['Date'], '/').getItem(0)) \n",
    "#     liquor_date_df=liquor_date_df.withColumn(\"Date\", to_date(\"Date\", \"MM/dd/yyyy\"))\n",
    "#     liquor_date_df=liquor_date_df.withColumn(\"week\", weekofyear(\"Date\"))\n",
    "      liquor_date_df=extract_date_columns_liquor(liquor_sales_df,\"Date\")\n",
    "    \n",
    "    \n",
    "    def get_lat_long(df,point_col):\n",
    "         df=df.withColumn(\"geometry_cleaned\", regexp_replace(point_col, \"POINT \\\\(\", \"\"))\\\n",
    "        .withColumn(\"geometry_cleaned\", regexp_replace(\"geometry_cleaned\", \"\\\\)\", \"\"))\\\n",
    "        .withColumn(\"lat\", split(col(\"geometry_cleaned\"), \" \")[1].cast(\"float\")) \\\n",
    "        .withColumn(\"lon\", split(col(\"geometry_cleaned\"), \" \")[0].cast(\"float\"))\\\n",
    "        .drop(\"geometry_cleaned\")\n",
    "        return df   \n",
    "            \n",
    "    liquor_date_df= get_lat_long(liquor_date_df,\"Store Location\")    \n",
    "    liquor_data_mart_df=missing_values_imputation(liquor_date_df)\n",
    "\n",
    "     \n",
    "    \n",
    "\n",
    "     \n",
    "#     liquor_date_df=liquor_date_df.withColumn(\"geometry_cleaned\", regexp_replace(\"Store Location\", \"POINT \\\\(\", \"\"))\\\n",
    "# .withColumn(\"geometry_cleaned\", regexp_replace(\"geometry_cleaned\", \"\\\\)\", \"\"))\\\n",
    "# .withColumn(\"lat\", split(col(\"geometry_cleaned\"), \" \")[1].cast(\"float\")) \\\n",
    "# .withColumn(\"lon\", split(col(\"geometry_cleaned\"), \" \")[0].cast(\"float\"))\\\n",
    "# .drop(\"geometry_cleaned\")\n",
    "    #liquor_date_df=missing_values_imputation(liquor_date_df)\n",
    " \n",
    "    \n",
    "\n",
    "    #### TRANSFORM ####\n",
    "\n",
    "\n",
    "    books_ratings_df=books_ratings_df.withColumn('Price',col('Price').cast(\"integer\"))\n",
    "    books_ratings_df = books_ratings_df.withColumn(\"review/time\", from_unixtime(\"review/time\"))\n",
    "  \n",
    "    # Joing the book data and books_ratings data\n",
    "    books_df_joined=books_df.join(books_ratings_df,books_df.Title ==  books_ratings_df.Title,\"inner\").drop(books_ratings_df.Title)\n",
    "    books_df_joined=missing_values_imputation(books_df_joined)\n",
    "\n",
    "    \n",
    "    \n",
    "    ####### Electronics transformation #######\n",
    "  \n",
    "    \n",
    "    df_sales=unionAll(*[sales_jan_df, sales_feb_df,sales_mar_df,sales_april_df,sales_may_df,sales_june_df\n",
    "           ,sales_july_df,sales_aug_df,sales_sep_df,sales_oct_df,sales_nov_df,sales_dec_df])\n",
    "    \n",
    "    ### SPLIT ADDRESS ###\n",
    "    \n",
    "    def split_address(df,address_col):\n",
    "        df = df.withColumn(\"city\", split(df[address_col],\",\").getItem(1))\\\n",
    "        .withColumn(\"state_code\", split(df[address_col],\",\").getItem(2))\n",
    "        df=df.withColumn(\"state\", split(df[\"state_code\"], \" \").getItem(1))\n",
    "        df = df.drop(\"state_code\")\n",
    "        return df\n",
    "   \n",
    "    df_sales=df_sales.withColumn(\"Order Date\", to_date(unix_timestamp(\"Order Date\", \"MM/dd/yy HH:mm\").cast(\"timestamp\")))\n",
    "    df_sales_split=split_address(df_sales,\"Purchase Address\")\n",
    "    \n",
    "#     split_df = df_sales.withColumn(\"city\", split(df_sales[\"Purchase Address\"],\",\").getItem(1))\\\n",
    "# .withColumn(\"state_code\", split(df_sales[\"Purchase Address\"],\",\").getItem(2))\n",
    "#     df_sales_split=split_df.withColumn(\"state\", split(split_df[\"state_code\"], \" \").getItem(1))\n",
    "#     df_sales_split = df_sales_split.drop(\"state_code\")\n",
    "    \n",
    "    ### date operations ###\n",
    "    def extract_date_columns_electronics(df,date_col):\n",
    "        df=df.withColumn('year', split(df[date_col], '-').getItem(0)) \\\n",
    "       .withColumn('month', split(df[date_col], '-').getItem(1)) \n",
    "        df = df.withColumn(\"orderDateOnly\", to_date(date_col))\n",
    "        df=df.withColumn(\"week\", weekofyear(date_col\"))\n",
    "        return df\n",
    "    df_date=extract_date_columns_electronics(df_sales_split,\"Order Date\")\n",
    "    \n",
    "#     df_date=df_sales_split.withColumn('year', split(df_sales_split['Order Date'], '-').getItem(0)) \\\n",
    "#        .withColumn('month', split(df_sales_split['Order Date'], '-').getItem(1)) \n",
    "#     df_date = df_date.withColumn(\"orderDateOnly\", to_date(\"Order Date\"))\n",
    "#     df_date=df_date.withColumn(\"week\", weekofyear(\"Order Date\"))\n",
    "    category_mapping = {\n",
    "    \"1\": \"Smartphone\",\n",
    "    \"2\": \"Charging Cable\",\n",
    "    \"3\": \"Headphones\",\n",
    "    \"4\": \"Monitor\",\n",
    "    \"5\": \"Batteries\",\n",
    "    \"6\": \"Laptop\",\n",
    "    \"7\": \"TV\",\n",
    "    \"8\": \"Dryer\",\n",
    "    \"9\": \"Washing Machine\",\n",
    "    \"10\": \"others\"\n",
    "}\n",
    "    df_date=df_date.withColumn(\"category\",\n",
    "                               when(col(\"product\").contains(\"Batteries\"), \"Batteries\")\n",
    "                               .when(col(\"product\").contains(\"Charging Cable\"), \"Charging Cable\")\n",
    "                               .when(col(\"product\").contains(\"Phone\"), \"Smartphone\")\n",
    "                               .when(col(\"product\").contains(\"Headphones\"), \"Headphones\")\n",
    "                               .when(col(\"product\").contains(\"Laptop\"), \"Laptop\")\n",
    "                               .when(col(\"product\").contains(\"TV\"), \"TV\")\n",
    "                               .when(col(\"product\").contains(\"Dryer\"), \"Dryer\")\n",
    "                               .when(col(\"product\").contains(\"Washing Machine\"), \"Washing Machine\")\n",
    "                               .otherwise(\"Others\"))\n",
    "    electronics_data_mart=missing_values_imputation(df_date)\n",
    "  \n",
    "    #### LOAD ####\n",
    "\n",
    "    #OUTPUT_LOCATION = '/output'\n",
    "\n",
    "    # save the above results as csv files in the bucket\n",
    "#     output_result(sum_price_by_title, OUTPUT_LOCATION, 'Title')\n",
    "#     output_result(count_price_by_title, OUTPUT_LOCATION, 'Title')\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec240ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87672e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_date_columns_liquor(df,date_col):\n",
    "    df=df.withColumn('year', split(liquor_sales_df[date_col], '/').getItem(2)) \\\n",
    "   .withColumn('month', split(liquor_sales_df[date_col], '/').getItem(0)) \n",
    "    df=df.withColumn(date_col, to_date(date_col, \"MM/dd/yyyy\"))\n",
    "    df=df.withColumn(\"week\", weekofyear(date_col))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "174cb649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+------------+--------------------+--------------------+---------------+--------+--------------------+-------------+-------------+--------+--------------------+-------------+--------------------+-----------+--------------------+----+------------------+-----------------+-------------------+------------+--------------+--------------------+---------------------+----+-----+----+\n",
      "|Invoice/Item Number|      Date|Store Number|          Store Name|             Address|           City|Zip Code|      Store Location|County Number|       County|Category|       Category Name|Vendor Number|         Vendor Name|Item Number|    Item Description|Pack|Bottle Volume (ml)|State Bottle Cost|State Bottle Retail|Bottles Sold|Sale (Dollars)|Volume Sold (Liters)|Volume Sold (Gallons)|year|month|week|\n",
      "+-------------------+----------+------------+--------------------+--------------------+---------------+--------+--------------------+-------------+-------------+--------+--------------------+-------------+--------------------+-----------+--------------------+----+------------------+-----------------+-------------------+------------+--------------+--------------------+---------------------+----+-----+----+\n",
      "|       S24127700024|2015-02-19|        3678|     Smoke Shop, The|     1918 SE 14TH ST|     DES MOINES|   50320|POINT (-93.597011...|           77|         Polk| 1031200|      VODKA FLAVORED|          380|Phillips Beverage...|      41783|Uv Blue Raspberry...|   6|               500|             4.89|               7.34|           2|         14.68|                 1.0|                 0.26|2015|   02|   8|\n",
      "|       S15066200002|2013-10-10|        2633|Hy-Vee #3 / BDI /...|     3221 SE 14TH ST|     DES MOINES|   50320|POINT (-93.596754...|           77|         Polk| 1082900|MISC. IMPORTED CO...|          305|             MHW Ltd|     904969|Sabe Premiom Sake...|   6|               750|            14.99|              22.49|           6|        134.94|                 4.5|                 1.19|2013|   10|  41|\n",
      "|       S19323500030|2014-06-03|        2607|Hy-Vee Wine and S...|      520 SO FREMONT|     SHENANDOAH|   51601|POINT (-95.385111...|           73|         Page| 1062200|PUERTO RICO & VIR...|          434|      Luxco-St Louis|      45277| Paramount White Rum|  12|              1000|             4.34|               6.51|          12|         78.12|                12.0|                 3.17|2014|   06|  23|\n",
      "|       S23334500013|2015-01-06|        4810|Kum & Go #518 / A...|3603 NE OTTERVIEW...|         ANKENY|   50021|POINT (-93.572458...|           77|         Polk| 1062200|PUERTO RICO & VIR...|           35|Bacardi U.S.A., Inc.|      43121|Bacardi Superior ...|  12|               500|             5.54|               8.31|           1|          8.31|                 0.5|                 0.13|2015|   01|   2|\n",
      "|       S09742200010|2012-12-27|        4025|Karam Kaur Khasri...|         702 13TH ST|   BELLE PLAINE|   52208|POINT (-92.277759...|            6|       Benton| 1012100|   CANADIAN WHISKIES|          260|     Diageo Americas|      11298|Crown Royal Canad...|   6|              1750|             31.0|              46.49|           2|         92.98|                 3.5|                 0.92|2012|   12|  52|\n",
      "|       S15034600007|2013-10-09|        4583|Kum & Go #5100 / ...|         208 MAIN ST|         MANSON|   50563|POINT (-94.534532...|           13|      Calhoun| 1081200|      CREAM LIQUEURS|          305|             MHW Ltd|      73050|            Rumchata|   6|               750|             12.5|              18.75|           6|         112.5|                 4.5|                 1.19|2013|   10|  41|\n",
      "|       S25185100053|2015-04-21|        5080|    C's Liquor Store|       719 2ND AVE W|        SPENCER|   51301|POINT (-95.147741...|           21|         Clay| 1081390|   IMPORTED SCHNAPPS|          421|   Sazerac Co., Inc.|      69713|Dr. McGillicuddy'...|  12|               500|             4.96|               7.44|           1|          7.44|                 0.5|                 0.13|2015|   04|  17|\n",
      "|       S25786400073|2015-05-21|        5102|      Wilkie Liquors|    724, 1st  ST  SE|   MOUNT VERNON|   52314|POINT (-91.410401...|           57|         Linn| 1031080|      VODKA 80 PROOF|          434|      Luxco-St Louis|      36307|       Hawkeye Vodka|  12|              1000|             4.05|               6.08|          24|        145.92|                24.0|                 6.34|2015|   05|  21|\n",
      "|       S26178600169|2015-06-15|        2506|Hy-Vee #1044 / Bu...|         3140 AGENCY|     BURLINGTON|   52601|POINT (-91.136655...|           29|   Des Moines| 1081600|     WHISKEY LIQUEUR|          260|     Diageo Americas|      66206|Piehole Cherry Pi...|  12|               500|              4.9|               7.35|           1|          7.35|                 0.5|                 0.13|2015|   06|  25|\n",
      "|       S11599200028|2013-04-11|        2630|Hy-Vee Drugstore ...|       1010  60TH ST|WEST DES MOINES|   50266|POINT (-93.790534...|           77|         Polk| 1071100|  AMERICAN COCKTAILS|          395|             Proximo|      58838|Jose Cuervo Authe...|   6|              1750|              8.2|               12.3|           6|          73.8|                10.5|                 2.77|2013|   04|  15|\n",
      "|       S14039300026|2013-08-21|        3916|Smokin' Joe's #5 ...|       1115 ALBIA RD|        OTTUMWA|   52501|POINT (-92.437224...|           90|      Wapello| 1012100|   CANADIAN WHISKIES|          260|     Diageo Americas|      11294|Crown Royal Canad...|  24|               375|             7.65|              11.48|           1|         11.48|                0.38|                  0.1|2013|   08|  34|\n",
      "|       S14777200004|2013-09-25|        4073|  Uptown Liquor, Llc|    306 HWY 69 SOUTH|    FOREST CITY|   50436|POINT (-93.633306...|           95|    Winnebago| 1012100|   CANADIAN WHISKIES|           55|Sazerac North Ame...|      12407| Canadian Ltd Whisky|  12|              1000|              5.5|               8.25|          12|          99.0|                12.0|                 3.17|2013|   09|  39|\n",
      "|       S28698700004|2015-10-27|        2578|Hy-Vee / Charles ...|        901 KELLY ST|   CHARLES CITY|   50616|POINT (-92.675560...|           34|        Floyd| 1031080|      VODKA 80 PROOF|          260|     Diageo Americas|      37426|Popov Vodka 80 Pr...|  12|               750|              4.5|               6.75|          12|          81.0|                 9.0|                 2.38|2015|   10|  44|\n",
      "|       S15825800006|2013-11-20|        4465| HOME TOWN FOOD ON 4|       714 S EAST ST|        POMEROY|   50575|POINT (-94.678054...|           13|      Calhoun| 1011100|    BLENDED WHISKIES|          434|      Luxco-St Louis|      24156|Hawkeye Blend Whi...|  12|               750|             3.36|               5.04|          24|        120.96|                18.0|                 4.76|2013|   11|  47|\n",
      "|       S27233900076|2015-08-10|        2613|Hy-Vee Food Store...|     2323 W BROADWAY| COUNCIL BLUFFS|   51501|POINT (-95.879662...|           78|Pottawattamie| 1022100|             TEQUILA|          410|The Patron Spirit...|      88296|Patron Tequila Si...|  12|               750|             27.0|               40.5|          12|         486.0|                 9.0|                 2.38|2015|   08|  33|\n",
      "|       S11716100052|2013-04-17|        3990|Cork and Bottle /...|      309 A AVE WEST|      OSKALOOSA|   52577|POINT (-92.648153...|           62|      Mahaska| 1032200|IMPORTED VODKA - ...|           35|Bacardi U.S.A., Inc.|      34436|Grey Goose Vodka ...|   6|               750|            17.97|              26.96|           1|         26.96|                0.75|                  0.2|2013|   04|  16|\n",
      "|       S28977300019|2015-11-10|        4743|No Frills Superma...| 1801 VALLEY VIEW DR| COUNCIL BLUFFS|   51503|POINT (-95.818775...|           78|Pottawattamie| 1022100|             TEQUILA|          395|             Proximo|      89198|Jose Cuervo Espec...|   6|              1750|            20.25|              30.38|           6|        182.28|                10.5|                 2.77|2015|   11|  46|\n",
      "|       S10005300010|2013-01-15|        2460|   Liquor Barn, Inc.|721 CENTRAL AVENU...|        HAMPTON|   50441|POINT (-93.21693 ...|           35|     Franklin| 1012100|   CANADIAN WHISKIES|           85|Brown-Forman Corp...|      12476|Canadian Mist Whi...|  12|               750|             5.44|               8.15|          12|          97.8|                 9.0|                 2.38|2013|   01|   3|\n",
      "|       S08028500005|2012-10-01|        4109|           Best Trip|         1516 SE 1ST|     DES MOINES|   50315|POINT (-93.613469...|           77|         Polk| 1011500|STRAIGHT RYE WHIS...|          255| Wilson Daniels Ltd.|      27102|       Templeton Rye|   6|               750|            18.08|              27.13|           6|        162.78|                 4.5|                 1.19|2012|   10|  40|\n",
      "|       S12260600026|2013-05-16|        3918|Smokin' Joe's #1 ...|  3120 ROCKINGHAM RD|      DAVENPORT|   52802|POINT (-90.621541...|           82|        Scott| 1081300| PEPPERMINT SCHNAPPS|          434|      Luxco-St Louis|      80576|Arrow Peppermint ...|  12|               750|             3.42|               5.13|           1|          5.13|                0.75|                  0.2|2013|   05|  20|\n",
      "+-------------------+----------+------------+--------------------+--------------------+---------------+--------+--------------------+-------------+-------------+--------+--------------------+-------------+--------------------+-----------+--------------------+----+------------------+-----------------+-------------------+------------+--------------+--------------------+---------------------+----+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_date_columns_liquor(liquor_sales_df,\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "595de9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+------------+--------------------+--------------------+---------------+--------+--------------------+-------------+-------------+--------+--------------------+-------------+--------------------+-----------+--------------------+----+------------------+-----------------+-------------------+------------+--------------+--------------------+---------------------+\n",
      "|Invoice/Item Number|      Date|Store Number|          Store Name|             Address|           City|Zip Code|      Store Location|County Number|       County|Category|       Category Name|Vendor Number|         Vendor Name|Item Number|    Item Description|Pack|Bottle Volume (ml)|State Bottle Cost|State Bottle Retail|Bottles Sold|Sale (Dollars)|Volume Sold (Liters)|Volume Sold (Gallons)|\n",
      "+-------------------+----------+------------+--------------------+--------------------+---------------+--------+--------------------+-------------+-------------+--------+--------------------+-------------+--------------------+-----------+--------------------+----+------------------+-----------------+-------------------+------------+--------------+--------------------+---------------------+\n",
      "|       S24127700024|02/19/2015|        3678|     Smoke Shop, The|     1918 SE 14TH ST|     DES MOINES|   50320|POINT (-93.597011...|           77|         Polk| 1031200|      VODKA FLAVORED|          380|Phillips Beverage...|      41783|Uv Blue Raspberry...|   6|               500|             4.89|               7.34|           2|         14.68|                 1.0|                 0.26|\n",
      "|       S15066200002|10/10/2013|        2633|Hy-Vee #3 / BDI /...|     3221 SE 14TH ST|     DES MOINES|   50320|POINT (-93.596754...|           77|         Polk| 1082900|MISC. IMPORTED CO...|          305|             MHW Ltd|     904969|Sabe Premiom Sake...|   6|               750|            14.99|              22.49|           6|        134.94|                 4.5|                 1.19|\n",
      "|       S19323500030|06/03/2014|        2607|Hy-Vee Wine and S...|      520 SO FREMONT|     SHENANDOAH|   51601|POINT (-95.385111...|           73|         Page| 1062200|PUERTO RICO & VIR...|          434|      Luxco-St Louis|      45277| Paramount White Rum|  12|              1000|             4.34|               6.51|          12|         78.12|                12.0|                 3.17|\n",
      "|       S23334500013|01/06/2015|        4810|Kum & Go #518 / A...|3603 NE OTTERVIEW...|         ANKENY|   50021|POINT (-93.572458...|           77|         Polk| 1062200|PUERTO RICO & VIR...|           35|Bacardi U.S.A., Inc.|      43121|Bacardi Superior ...|  12|               500|             5.54|               8.31|           1|          8.31|                 0.5|                 0.13|\n",
      "|       S09742200010|12/27/2012|        4025|Karam Kaur Khasri...|         702 13TH ST|   BELLE PLAINE|   52208|POINT (-92.277759...|            6|       Benton| 1012100|   CANADIAN WHISKIES|          260|     Diageo Americas|      11298|Crown Royal Canad...|   6|              1750|             31.0|              46.49|           2|         92.98|                 3.5|                 0.92|\n",
      "|       S15034600007|10/09/2013|        4583|Kum & Go #5100 / ...|         208 MAIN ST|         MANSON|   50563|POINT (-94.534532...|           13|      Calhoun| 1081200|      CREAM LIQUEURS|          305|             MHW Ltd|      73050|            Rumchata|   6|               750|             12.5|              18.75|           6|         112.5|                 4.5|                 1.19|\n",
      "|       S25185100053|04/21/2015|        5080|    C's Liquor Store|       719 2ND AVE W|        SPENCER|   51301|POINT (-95.147741...|           21|         Clay| 1081390|   IMPORTED SCHNAPPS|          421|   Sazerac Co., Inc.|      69713|Dr. McGillicuddy'...|  12|               500|             4.96|               7.44|           1|          7.44|                 0.5|                 0.13|\n",
      "|       S25786400073|05/21/2015|        5102|      Wilkie Liquors|    724, 1st  ST  SE|   MOUNT VERNON|   52314|POINT (-91.410401...|           57|         Linn| 1031080|      VODKA 80 PROOF|          434|      Luxco-St Louis|      36307|       Hawkeye Vodka|  12|              1000|             4.05|               6.08|          24|        145.92|                24.0|                 6.34|\n",
      "|       S26178600169|06/15/2015|        2506|Hy-Vee #1044 / Bu...|         3140 AGENCY|     BURLINGTON|   52601|POINT (-91.136655...|           29|   Des Moines| 1081600|     WHISKEY LIQUEUR|          260|     Diageo Americas|      66206|Piehole Cherry Pi...|  12|               500|              4.9|               7.35|           1|          7.35|                 0.5|                 0.13|\n",
      "|       S11599200028|04/11/2013|        2630|Hy-Vee Drugstore ...|       1010  60TH ST|WEST DES MOINES|   50266|POINT (-93.790534...|           77|         Polk| 1071100|  AMERICAN COCKTAILS|          395|             Proximo|      58838|Jose Cuervo Authe...|   6|              1750|              8.2|               12.3|           6|          73.8|                10.5|                 2.77|\n",
      "|       S14039300026|08/21/2013|        3916|Smokin' Joe's #5 ...|       1115 ALBIA RD|        OTTUMWA|   52501|POINT (-92.437224...|           90|      Wapello| 1012100|   CANADIAN WHISKIES|          260|     Diageo Americas|      11294|Crown Royal Canad...|  24|               375|             7.65|              11.48|           1|         11.48|                0.38|                  0.1|\n",
      "|       S14777200004|09/25/2013|        4073|  Uptown Liquor, Llc|    306 HWY 69 SOUTH|    FOREST CITY|   50436|POINT (-93.633306...|           95|    Winnebago| 1012100|   CANADIAN WHISKIES|           55|Sazerac North Ame...|      12407| Canadian Ltd Whisky|  12|              1000|              5.5|               8.25|          12|          99.0|                12.0|                 3.17|\n",
      "|       S28698700004|10/27/2015|        2578|Hy-Vee / Charles ...|        901 KELLY ST|   CHARLES CITY|   50616|POINT (-92.675560...|           34|        Floyd| 1031080|      VODKA 80 PROOF|          260|     Diageo Americas|      37426|Popov Vodka 80 Pr...|  12|               750|              4.5|               6.75|          12|          81.0|                 9.0|                 2.38|\n",
      "|       S15825800006|11/20/2013|        4465| HOME TOWN FOOD ON 4|       714 S EAST ST|        POMEROY|   50575|POINT (-94.678054...|           13|      Calhoun| 1011100|    BLENDED WHISKIES|          434|      Luxco-St Louis|      24156|Hawkeye Blend Whi...|  12|               750|             3.36|               5.04|          24|        120.96|                18.0|                 4.76|\n",
      "|       S27233900076|08/10/2015|        2613|Hy-Vee Food Store...|     2323 W BROADWAY| COUNCIL BLUFFS|   51501|POINT (-95.879662...|           78|Pottawattamie| 1022100|             TEQUILA|          410|The Patron Spirit...|      88296|Patron Tequila Si...|  12|               750|             27.0|               40.5|          12|         486.0|                 9.0|                 2.38|\n",
      "|       S11716100052|04/17/2013|        3990|Cork and Bottle /...|      309 A AVE WEST|      OSKALOOSA|   52577|POINT (-92.648153...|           62|      Mahaska| 1032200|IMPORTED VODKA - ...|           35|Bacardi U.S.A., Inc.|      34436|Grey Goose Vodka ...|   6|               750|            17.97|              26.96|           1|         26.96|                0.75|                  0.2|\n",
      "|       S28977300019|11/10/2015|        4743|No Frills Superma...| 1801 VALLEY VIEW DR| COUNCIL BLUFFS|   51503|POINT (-95.818775...|           78|Pottawattamie| 1022100|             TEQUILA|          395|             Proximo|      89198|Jose Cuervo Espec...|   6|              1750|            20.25|              30.38|           6|        182.28|                10.5|                 2.77|\n",
      "|       S10005300010|01/15/2013|        2460|   Liquor Barn, Inc.|721 CENTRAL AVENU...|        HAMPTON|   50441|POINT (-93.21693 ...|           35|     Franklin| 1012100|   CANADIAN WHISKIES|           85|Brown-Forman Corp...|      12476|Canadian Mist Whi...|  12|               750|             5.44|               8.15|          12|          97.8|                 9.0|                 2.38|\n",
      "|       S08028500005|10/01/2012|        4109|           Best Trip|         1516 SE 1ST|     DES MOINES|   50315|POINT (-93.613469...|           77|         Polk| 1011500|STRAIGHT RYE WHIS...|          255| Wilson Daniels Ltd.|      27102|       Templeton Rye|   6|               750|            18.08|              27.13|           6|        162.78|                 4.5|                 1.19|\n",
      "|       S12260600026|05/16/2013|        3918|Smokin' Joe's #1 ...|  3120 ROCKINGHAM RD|      DAVENPORT|   52802|POINT (-90.621541...|           82|        Scott| 1081300| PEPPERMINT SCHNAPPS|          434|      Luxco-St Louis|      80576|Arrow Peppermint ...|  12|               750|             3.42|               5.13|           1|          5.13|                0.75|                  0.2|\n",
      "+-------------------+----------+------------+--------------------+--------------------+---------------+--------+--------------------+-------------+-------------+--------+--------------------+-------------+--------------------+-----------+--------------------+----+------------------+-----------------+-------------------+------------+--------------+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#liquor_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f311188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59875bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76bc805",
   "metadata": {},
   "outputs": [],
   "source": [
    "liquor_date_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20a6d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, isnan, count, col, monotonically_increasing_id, round\n",
    "\n",
    "def find_row_missing_percentage(df):\n",
    "    \"\"\"\n",
    "    This function takes a PySpark DataFrame as input and returns a dataframe after adding \n",
    "    columns that have missing values and the percentage of missing values in each row.\n",
    "\n",
    "    :param df: PySpark DataFrame\n",
    "    :return: PySpark DataFrame with percentage missing in each row\n",
    "             List containing percentage of missing rows\n",
    "    \"\"\"\n",
    "    # Count the number of columns\n",
    "    total_columns = len(df.columns)\n",
    "\n",
    "    # Find rows with missing values and count missing values in each row\n",
    "    missing_values_df = df.select(\"*\",\n",
    "        sum(when(isnan(col(column_name)) | col(column_name).isNull(), 1).otherwise(0)\n",
    "            for column_name in df.columns).alias(\"missing_count\"))\n",
    "\n",
    "    # Calculate the percentage of missing values for each row\n",
    "    missing_percent_df = missing_values_df.withColumn(\"percent_missing\", col(\"missing_count\") / total_columns * 100)\n",
    "\n",
    "    missing_percent_df = missing_percent_df.withColumn(\"percent_missing\", round(missing_percent_df[\"percent_missing\"], 0))\n",
    "\n",
    "    # Calculate percentage of missing rows in dataframe \n",
    "    No_of_missRows = missing_percent_df.select('missing_count').where(missing_percent_df.missing_count>0).count()\n",
    "    Percent_miss_rows = No_of_missRows/df.count()\n",
    "\n",
    "    return missing_percent_df, Percent_miss_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c0859e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Invoice/Item Number: string, Store Number: string, Store Name: string, Address: string, City: string, Zip Code: string, Store Location: string, County Number: string, County: string, Category: string, Category Name: string, Vendor Number: string, Vendor Name: string, Item Number: string, Item Description: string, Pack: string, Bottle Volume (ml): string, State Bottle Cost: string, State Bottle Retail: string, Bottles Sold: string, Sale (Dollars): string, Volume Sold (Liters): string, Volume Sold (Gallons): string, year: string, month: string, week: string, lat: string, lon: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liquor_date_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bca945c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|      Date|week|\n",
      "+----------+----+\n",
      "|2015-02-19|   8|\n",
      "|2013-10-10|  41|\n",
      "|2014-06-03|  23|\n",
      "|2015-01-06|   2|\n",
      "|2012-12-27|  52|\n",
      "|2013-10-09|  41|\n",
      "|2015-04-21|  17|\n",
      "|2015-05-21|  21|\n",
      "|2015-06-15|  25|\n",
      "|2013-04-11|  15|\n",
      "|2013-08-21|  34|\n",
      "|2013-09-25|  39|\n",
      "|2015-10-27|  44|\n",
      "|2013-11-20|  47|\n",
      "|2015-08-10|  33|\n",
      "|2013-04-17|  16|\n",
      "|2015-11-10|  46|\n",
      "|2013-01-15|   3|\n",
      "|2012-10-01|  40|\n",
      "|2013-05-16|  20|\n",
      "+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "liquor_date_df.select(\"Date\",\"week\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b3be9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|      lat|       lon|\n",
      "+---------+----------+\n",
      "|41.570843| -93.59701|\n",
      "|  41.5541|-93.596756|\n",
      "|40.761738| -95.38511|\n",
      "| 41.76099| -93.57246|\n",
      "|41.897053|-92.277756|\n",
      "|42.517857| -94.53453|\n",
      "| 43.14521| -95.14774|\n",
      "|41.918327|  -91.4104|\n",
      "|40.814667| -91.13666|\n",
      "| 41.58498|-93.790535|\n",
      "|41.009342|-92.437225|\n",
      "| 43.26154| -93.63331|\n",
      "|43.066994| -92.67556|\n",
      "| 42.54299|-94.678055|\n",
      "|41.261925| -95.87966|\n",
      "|41.296227|-92.648155|\n",
      "| 41.24394| -95.81877|\n",
      "| 42.74173| -93.21693|\n",
      "|41.575928| -93.61347|\n",
      "|41.510082|-90.621544|\n",
      "+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "liquor_date_df=liquor_date_df.withColumn(\"geometry_cleaned\", regexp_replace(\"Store Location\", \"POINT \\\\(\", \"\"))\\\n",
    ".withColumn(\"geometry_cleaned\", regexp_replace(\"geometry_cleaned\", \"\\\\)\", \"\"))\\\n",
    ".withColumn(\"lat\", split(col(\"geometry_cleaned\"), \" \")[1].cast(\"float\")) \\\n",
    ".withColumn(\"lon\", split(col(\"geometry_cleaned\"), \" \")[0].cast(\"float\"))\\\n",
    ".drop(\"geometry_cleaned\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01c6d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "liquor_date_df=liquor_sales_df.withColumn('year', split(liquor_sales_df['Date'], '/').getItem(2)) \\\n",
    "   .withColumn('month', split(liquor_sales_df['Date'], '/').getItem(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fa41f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|year|month|\n",
      "+----+-----+\n",
      "|2015|   02|\n",
      "+----+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "liquor_date_df.select(\"year\",\"month\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ff31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "liquor_sales_df = liquor_sales_df.withColumn(\"orderDateOnly\", to_date(\"Order Date\"))\n",
    "liquor_sales_df=liquor_sales_df.withColumn(\"week\", weekofyear(\"Order Date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c4053c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186850"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_date.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95eb629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Electronic sales daily,weekly monthly,yearly\n",
    "\n",
    "# df_date.groupBy(\"orderDateOnly\", \"Product\") \\\n",
    "#     .agg(round(sum(col(\"Quantity Ordered\") * col(\"Price Each\")), 2).alias(\"salesAmount\")) \\\n",
    "#     .show()\n",
    "\n",
    "\n",
    "\n",
    "# df_date.groupBy(\"month\", \"Product\") \\\n",
    "#     .agg(round(sum(col(\"Quantity Ordered\") * col(\"Price Each\")), 2).alias(\"salesAmount\")) \\\n",
    "#     .show()\n",
    "\n",
    "\n",
    "# df_date.groupBy(\"year\", \"Product\") \\\n",
    "#     .agg(round(sum(col(\"Quantity Ordered\") * col(\"Price Each\")), 2).alias(\"salesAmount\")) \\\n",
    "#     .show()\n",
    "\n",
    "\n",
    "# df_date.groupBy(\"week\", \"Product\") \\\n",
    "# .agg(round(sum(col(\"Quantity Ordered\") * col(\"Price Each\")), 2).alias(\"salesAmount\")) \\\n",
    "#     .show()\n",
    "\n",
    "\n",
    "# top_products = df_date.groupBy('Product') \\\n",
    "#                  .agg(round(sum(col(\"Quantity Ordered\") * col(\"Price Each\")), 2).alias('total_sales')) \\\n",
    "#                  .orderBy(desc('total_sales')) \\\n",
    "#                  .limit(10)\n",
    "\n",
    "# top_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a9df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split() with withColumn\n",
    "# df_date=df_sales_split.withColumn('year', split(df_sales_split['Order Date'], '-').getItem(0)) \\\n",
    "#        .withColumn('month', split(df_sales_split['Order Date'], '-').getItem(1)) \n",
    "# df_date = df_date.withColumn(\"orderDateOnly\", to_date(\"Order Date\"))\n",
    "\n",
    "# df_date.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "de0e7ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df = df_sales.withColumn(\"city\", split(df_sales[\"Purchase Address\"],\",\").getItem(1))\\\n",
    ".withColumn(\"state_code\", split(df_sales[\"Order Date Address\"],\",\").getItem(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e483c77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+--------------+-----+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|          city|state|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+--------------+-----+\n",
      "|  141234|              iPhone|               1|     700.0|01/22/19 21:25|944 Walnut St, Bo...|        Boston|   MA|\n",
      "|  141235|Lightning Chargin...|               1|     14.95|01/28/19 14:15|185 Maple St, Por...|      Portland|   OR|\n",
      "|  141236|    Wired Headphones|               2|     11.99|01/17/19 13:33|538 Adams St, San...| San Francisco|   CA|\n",
      "|  141237|    27in FHD Monitor|               1|    149.99|01/05/19 20:33|738 10th St, Los ...|   Los Angeles|   CA|\n",
      "|  141238|    Wired Headphones|               1|     11.99|01/25/19 11:59|387 10th St, Aust...|        Austin|   TX|\n",
      "|  141239|AAA Batteries (4-...|               1|      2.99|01/29/19 20:22|775 Willow St, Sa...| San Francisco|   CA|\n",
      "|  141240|27in 4K Gaming Mo...|               1|    389.99|01/26/19 12:16|979 Park St, Los ...|   Los Angeles|   CA|\n",
      "|  141241|USB-C Charging Cable|               1|     11.95|01/05/19 12:04|181 6th St, San F...| San Francisco|   CA|\n",
      "|  141242|Bose SoundSport H...|               1|     99.99|01/01/19 10:30|867 Willow St, Lo...|   Los Angeles|   CA|\n",
      "|  141243|Apple Airpods Hea...|               1|     150.0|01/22/19 21:20|657 Johnson St, S...| San Francisco|   CA|\n",
      "|  141244|Apple Airpods Hea...|               1|     150.0|01/07/19 11:29|492 Walnut St, Sa...| San Francisco|   CA|\n",
      "|  141245|  Macbook Pro Laptop|               1|    1700.0|01/31/19 10:12|322 6th St, San F...| San Francisco|   CA|\n",
      "|  141246|AAA Batteries (4-...|               3|      2.99|01/09/19 18:57|618 7th St, Los A...|   Los Angeles|   CA|\n",
      "|  141247|    27in FHD Monitor|               1|    149.99|01/25/19 19:19|512 Wilson St, Sa...| San Francisco|   CA|\n",
      "|  141248|       Flatscreen TV|               1|     300.0|01/03/19 21:54|363 Spruce St, Au...|        Austin|   TX|\n",
      "|  141249|    27in FHD Monitor|               1|    149.99|01/05/19 17:20|440 Cedar St, Por...|      Portland|   OR|\n",
      "|  141250|     Vareebadd Phone|               1|     400.0|01/10/19 11:20|471 Center St, Lo...|   Los Angeles|   CA|\n",
      "|  141251|Apple Airpods Hea...|               1|     150.0|01/24/19 08:13|414 Walnut St, Bo...|        Boston|   MA|\n",
      "|  141252|USB-C Charging Cable|               1|     11.95|01/30/19 09:28|220 9th St, Los A...|   Los Angeles|   CA|\n",
      "|  141253|AA Batteries (4-p...|               1|      3.84|01/17/19 00:09|385 11th St, Atla...|       Atlanta|   GA|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales_split.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d235304f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+----------+--------------------+--------------+-----+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|Order Date|    Purchase Address|          city|state|\n",
      "+--------+--------------------+----------------+----------+----------+--------------------+--------------+-----+\n",
      "|  141234|              iPhone|               1|     700.0|2019-01-22|944 Walnut St, Bo...|        Boston|   MA|\n",
      "|  141235|Lightning Chargin...|               1|     14.95|2019-01-28|185 Maple St, Por...|      Portland|   OR|\n",
      "|  141236|    Wired Headphones|               2|     11.99|2019-01-17|538 Adams St, San...| San Francisco|   CA|\n",
      "|  141237|    27in FHD Monitor|               1|    149.99|2019-01-05|738 10th St, Los ...|   Los Angeles|   CA|\n",
      "|  141238|    Wired Headphones|               1|     11.99|2019-01-25|387 10th St, Aust...|        Austin|   TX|\n",
      "|  141239|AAA Batteries (4-...|               1|      2.99|2019-01-29|775 Willow St, Sa...| San Francisco|   CA|\n",
      "|  141240|27in 4K Gaming Mo...|               1|    389.99|2019-01-26|979 Park St, Los ...|   Los Angeles|   CA|\n",
      "|  141241|USB-C Charging Cable|               1|     11.95|2019-01-05|181 6th St, San F...| San Francisco|   CA|\n",
      "|  141242|Bose SoundSport H...|               1|     99.99|2019-01-01|867 Willow St, Lo...|   Los Angeles|   CA|\n",
      "|  141243|Apple Airpods Hea...|               1|     150.0|2019-01-22|657 Johnson St, S...| San Francisco|   CA|\n",
      "|  141244|Apple Airpods Hea...|               1|     150.0|2019-01-07|492 Walnut St, Sa...| San Francisco|   CA|\n",
      "|  141245|  Macbook Pro Laptop|               1|    1700.0|2019-01-31|322 6th St, San F...| San Francisco|   CA|\n",
      "|  141246|AAA Batteries (4-...|               3|      2.99|2019-01-09|618 7th St, Los A...|   Los Angeles|   CA|\n",
      "|  141247|    27in FHD Monitor|               1|    149.99|2019-01-25|512 Wilson St, Sa...| San Francisco|   CA|\n",
      "|  141248|       Flatscreen TV|               1|     300.0|2019-01-03|363 Spruce St, Au...|        Austin|   TX|\n",
      "|  141249|    27in FHD Monitor|               1|    149.99|2019-01-05|440 Cedar St, Por...|      Portland|   OR|\n",
      "|  141250|     Vareebadd Phone|               1|     400.0|2019-01-10|471 Center St, Lo...|   Los Angeles|   CA|\n",
      "|  141251|Apple Airpods Hea...|               1|     150.0|2019-01-24|414 Walnut St, Bo...|        Boston|   MA|\n",
      "|  141252|USB-C Charging Cable|               1|     11.95|2019-01-30|220 9th St, Los A...|   Los Angeles|   CA|\n",
      "|  141253|AA Batteries (4-p...|               1|      3.84|2019-01-17|385 11th St, Atla...|       Atlanta|   GA|\n",
      "+--------+--------------------+----------------+----------+----------+--------------------+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales_split.withColumn(\"Order Date\", to_date(unix_timestamp(\"Order Date\", \"MM/dd/yy HH:mm\").cast(\"timestamp\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a2ea903e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------------+----------+--------------+--------------------+-------+-----+\n",
      "|Order ID|Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|   city|state|\n",
      "+--------+-------+----------------+----------+--------------+--------------------+-------+-----+\n",
      "|  141234| iPhone|               1|     700.0|01/22/19 21:25|944 Walnut St, Bo...| Boston|   MA|\n",
      "+--------+-------+----------------+----------+--------------+--------------------+-------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales_split.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b997ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = df.withColumn('year', split(df['dob'], '-').getItem(0)) \\\n",
    "#        .withColumn('month', split(df['dob'], '-').getItem(1)) \\\n",
    "#        .withColumn('day', split(df['dob'], '-').getItem(2))\n",
    "# df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "059874e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------+--------------------+--------------------+---------+---------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Order Date|    Purchase Address|        split_column|     city|    state|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+--------------------+---------+---------+\n",
      "|  141234|              iPhone|               1|     700.0|01/22/19 21:25|944 Walnut St, Bo...|[944 Walnut St,  ...|   Boston| MA 02215|\n",
      "|  141235|Lightning Chargin...|               1|     14.95|01/28/19 14:15|185 Maple St, Por...|[185 Maple St,  P...| Portland| OR 97035|\n",
      "+--------+--------------------+----------------+----------+--------------+--------------------+--------------------+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bf6b830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales1=df_sales.select(split(df_sales[\"Purchase Address\"], \",\").alias(\"city\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f19de909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[split(Purchase Address, ,, -1) AS city[1]: string]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "95f2c190",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|split(Purchase Address, ,, -1) AS split_columns[2]|\n",
      "+--------------------------------------------------+\n",
      "|                                          MA 02215|\n",
      "+--------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.select(split(df_sales[\"Purchase Address\"], \",\").alias(\"split_columns\")[2]).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "22f0f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "books_ratings_df = books_ratings_df.withColumn(\"review/time\", from_unixtime(\"review/time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "805652d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+-------------+--------------------+------------------+------------+-------------------+--------------------+--------------------+\n",
      "|        Id|               Title|Price|      User_id|         profileName|review/helpfulness|review/score|        review/time|      review/summary|         review/text|\n",
      "+----------+--------------------+-----+-------------+--------------------+------------------+------------+-------------------+--------------------+--------------------+\n",
      "|1882931173|Its Only Art If I...| null|AVCGYZL8FQQTD|\"Jim of Oz \"\"jim-...|               7/7|         4.0|1999-10-23 08:00:00|Nice collection o...|This is only for ...|\n",
      "+----------+--------------------+-----+-------------+--------------------+------------------+------------+-------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_ratings_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78098ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title',\n",
       " 'description',\n",
       " 'authors',\n",
       " 'image',\n",
       " 'previewLink',\n",
       " 'publisher',\n",
       " 'publishedDate',\n",
       " 'infoLink',\n",
       " 'categories',\n",
       " 'ratingsCount',\n",
       " 'Id',\n",
       " 'Price',\n",
       " 'User_id',\n",
       " 'profileName',\n",
       " 'review/helpfulness',\n",
       " 'review/score',\n",
       " 'review/time',\n",
       " 'review/summary',\n",
       " 'review/text']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "books_df.join(books_ratings_df,books_df.Title ==  books_ratings_df.Title,\"inner\").drop(books_ratings_df.Title).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "689818fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999829"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df_joined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "889bd6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#books_df_joined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c0fd5fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212404"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df.count()#212404\n",
    "books_ratings_df.count()#3000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f87dc014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Invoice/Item Number: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Store Number: integer (nullable = true)\n",
      " |-- Store Name: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zip Code: string (nullable = true)\n",
      " |-- Store Location: string (nullable = true)\n",
      " |-- County Number: integer (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- Category: integer (nullable = true)\n",
      " |-- Category Name: string (nullable = true)\n",
      " |-- Vendor Number: integer (nullable = true)\n",
      " |-- Vendor Name: string (nullable = true)\n",
      " |-- Item Number: string (nullable = true)\n",
      " |-- Item Description: string (nullable = true)\n",
      " |-- Pack: integer (nullable = true)\n",
      " |-- Bottle Volume (ml): integer (nullable = true)\n",
      " |-- State Bottle Cost: double (nullable = true)\n",
      " |-- State Bottle Retail: double (nullable = true)\n",
      " |-- Bottles Sold: integer (nullable = true)\n",
      " |-- Sale (Dollars): double (nullable = true)\n",
      " |-- Volume Sold (Liters): double (nullable = true)\n",
      " |-- Volume Sold (Gallons): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "liquor_sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5315f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mapping = {\n",
    "    \"1\": \"Smartphone\",\n",
    "    \"2\": \"Charging Cable\",\n",
    "    \"3\": \"Headphones\",\n",
    "    \"4\": \"Monitor\",\n",
    "    \"5\": \"Batteries\",\n",
    "    \"6\": \"Laptop\",\n",
    "    \"7\": \"TV\",\n",
    "    \"8\": \"Dryer\",\n",
    "    \"9\": \"Washing Machine\",\n",
    "    \"10\": \"others\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640fe0a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'Smartphone',\n",
       " 2: 'Charging Cable',\n",
       " 3: 'Headphones',\n",
       " 4: 'Monitor',\n",
       " 5: 'Batteries',\n",
       " 6: 'Laptop',\n",
       " 7: 'TV',\n",
       " 8: 'Dryer',\n",
       " 9: 'Washing Machine',\n",
       " 10: 'others'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f55693a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when_conditions = []\n",
    "# for category_id, category_name in category_mapping.items():\n",
    "#     when_conditions.append(when(col(\"product\").contains(category_name), category_name))\n",
    "\n",
    "# mapped_df = df_date.withColumn(\"category\", \n",
    "#                                   when(*when_conditions)\n",
    "#                                   .otherwise(\"Others\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b0ed7989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|      category|             product|\n",
      "+--------------+--------------------+\n",
      "|    Smartphone|              iPhone|\n",
      "|Charging Cable|Lightning Chargin...|\n",
      "|    Headphones|    Wired Headphones|\n",
      "|        Others|    27in FHD Monitor|\n",
      "|    Headphones|    Wired Headphones|\n",
      "|     Batteries|AAA Batteries (4-...|\n",
      "|        Others|27in 4K Gaming Mo...|\n",
      "|Charging Cable|USB-C Charging Cable|\n",
      "|    Headphones|Bose SoundSport H...|\n",
      "|    Headphones|Apple Airpods Hea...|\n",
      "|    Headphones|Apple Airpods Hea...|\n",
      "|        Laptop|  Macbook Pro Laptop|\n",
      "|     Batteries|AAA Batteries (4-...|\n",
      "|        Others|    27in FHD Monitor|\n",
      "|            TV|       Flatscreen TV|\n",
      "|        Others|    27in FHD Monitor|\n",
      "|    Smartphone|     Vareebadd Phone|\n",
      "|    Headphones|Apple Airpods Hea...|\n",
      "|Charging Cable|USB-C Charging Cable|\n",
      "|     Batteries|AA Batteries (4-p...|\n",
      "+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date.withColumn(\"category\",\n",
    "                               when(col(\"product\").contains(\"Batteries\"), \"Batteries\")\n",
    "                               .when(col(\"product\").contains(\"Charging Cable\"), \"Charging Cable\")\n",
    "                               .when(col(\"product\").contains(\"Phone\"), \"Smartphone\")\n",
    "                               .when(col(\"product\").contains(\"Headphones\"), \"Headphones\")\n",
    "                               .when(col(\"product\").contains(\"Laptop\"), \"Laptop\")\n",
    "                               .when(col(\"product\").contains(\"TV\"), \"TV\")\n",
    "                               .when(col(\"product\").contains(\"Dryer\"), \"Dryer\")\n",
    "                               .when(col(\"product\").contains(\"Washing Machine\"), \"Washing Machine\")\n",
    "                               .otherwise(\"Others\")).select(\"category\",\"product\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d57faf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|http://books.goog...|    3|\n",
      "|                    |    2|\n",
      "| a service for wh...|    2|\n",
      "|http://books.goog...|    2|\n",
      "|http://books.goog...|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_df_joined.limit(10).select(explode(split(col(\"categories\"), \", \")).alias(\"word\")) \\\n",
    "    .groupBy(\"word\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52374595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28362"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df_joined.select(\"categories\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2538e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a6cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fae1396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_date.select(\"Product\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f908a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_jan_df= read_in_data(spark, 'Sales_January_2019.csv')\n",
    "# sales_feb_df= read_in_data(spark, 'Sales_February_2019.csv')\n",
    "# sales_mar_df= read_in_data(spark, 'Sales_March_2019.csv')\n",
    "# sales_april_df= read_in_data(spark, 'Sales_April_2019.csv')\n",
    "# sales_may_df= read_in_data(spark, 'Sales_May_2019.csv')\n",
    "# sales_june_df= read_in_data(spark, 'Sales_June_2019.csv')\n",
    "# sales_july_df= read_in_data(spark, 'Sales_July_2019.csv')\n",
    "# sales_aug_df= read_in_data(spark, 'Sales_August_2019.csv')\n",
    "# sales_sep_df= read_in_data(spark, 'Sales_September_2019.csv')\n",
    "# sales_oct_df= read_in_data(spark, 'Sales_October_2019.csv')\n",
    "# sales_nov_df= read_in_data(spark, 'Sales_November_2019.csv')\n",
    "# sales_dec_df= read_in_data(spark, 'Sales_December_2019.csv')\n",
    "\n",
    "\n",
    "# def unionAll(*dfs):\n",
    "#     return reduce(DataFrame.unionAll, dfs) \n",
    "\n",
    "# df_sales=unionAll(*[sales_jan_df, sales_feb_df,sales_mar_df,sales_april_df,sales_may_df,sales_june_df\n",
    "#            ,sales_july_df,sales_aug_df,sales_sep_df,sales_oct_df,sales_nov_df,sales_dec_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38170fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Order ID: string, Product: string, Quantity Ordered: string, Price Each: string, Order Date: string, Purchase Address: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_sales.describe()\n",
    "#df_sales.select('Order Date').distinct().collect()\n",
    "#df_sales.show().limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8d211",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#liquor_sales_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d9c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER_ADDR_port = \"income-mysql-db.cjuyq4k026ji.us-east-2.rds.amazonaws.com:3306\"\n",
    "db_name = \"income\"\n",
    "f\"jdbc:mysql://{SERVER_ADDR_port}/{db_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3067760",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_ratings_df.limit(10).write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"url\", f\"jdbc:mysql://{SERVER_ADDR_port}/{db_name}\") \\\n",
    "  .option(\"dbtable\", \"books_rating\") \\\n",
    "  .option(\"user\", \"admin\") \\\n",
    "  .option(\"password\", \"Logindatabase24\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "154842c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "liquor_sales_df.limit(10).write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"url\", f\"jdbc:mysql://{SERVER_ADDR_port}/{db_name}\") \\\n",
    "  .option(\"dbtable\", \"liquor_sales\") \\\n",
    "  .option(\"user\", \"admin\") \\\n",
    "  .option(\"password\", \"Logindatabase24\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e306637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.limit(10).write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"url\", f\"jdbc:mysql://{SERVER_ADDR_port}/{db_name}\") \\\n",
    "  .option(\"dbtable\", \"sales_data\") \\\n",
    "  .option(\"user\", \"admin\") \\\n",
    "  .option(\"password\", \"Logindatabase24\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "369b55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER_ADDR_port = \"income-mysql-db.cjuyq4k026ji.us-east-2.rds.amazonaws.com:3306\"\n",
    "db_name = \"income\"\n",
    "\n",
    "books_df.limit(100).write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"url\", f\"jdbc:mysql://{SERVER_ADDR_port}/{db_name}\") \\\n",
    "  .option(\"dbtable\", \"books_data\") \\\n",
    "  .option(\"user\", \"admin\") \\\n",
    "  .option(\"password\", \"Logindatabase24\") \\\n",
    "  .save()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f230e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4002b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Electronics data mart\n",
    "SERVER_ADDR_port = \"income-mysql-db.cjuyq4k026ji.us-east-2.rds.amazonaws.com:3306\"\n",
    "db_name = \"income\"\n",
    "\n",
    "df_date_electronics.limit(1000).write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"url\", f\"jdbc:mysql://{SERVER_ADDR_port}/{db_name}\") \\\n",
    "  .option(\"dbtable\", \"electronicsDataMart\") \\\n",
    "  .option(\"user\", \"admin\") \\\n",
    "  .option(\"password\", \"Logindatabase24\") \\\n",
    "  .save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "643d15af",
   "metadata": {},
   "outputs": [],
   "source": [
    "##liquor data mart\n",
    "SERVER_ADDR_port = \"income-mysql-db.cjuyq4k026ji.us-east-2.rds.amazonaws.com:3306\"\n",
    "db_name = \"income\"\n",
    "\n",
    "liquor_date_df.limit(30).write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"url\", f\"jdbc:mysql://{SERVER_ADDR_port}/{db_name}\") \\\n",
    "  .option(\"dbtable\", \"liquorDataMart\") \\\n",
    "  .option(\"user\", \"admin\") \\\n",
    "  .option(\"password\", \"Logindatabase24\") \\\n",
    "  .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca92ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999829"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df_joined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba7dcde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title',\n",
       " 'description',\n",
       " 'authors',\n",
       " 'image',\n",
       " 'previewLink',\n",
       " 'publisher',\n",
       " 'publishedDate',\n",
       " 'infoLink',\n",
       " 'categories',\n",
       " 'ratingsCount',\n",
       " 'Id',\n",
       " 'Title',\n",
       " 'Price',\n",
       " 'User_id',\n",
       " 'profileName',\n",
       " 'review/helpfulness',\n",
       " 'review/score',\n",
       " 'review/time',\n",
       " 'review/summary',\n",
       " 'review/text']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df_joined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d82ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Books data mart\n",
    "SERVER_ADDR_port = \"income-mysql-db.cjuyq4k026ji.us-east-2.rds.amazonaws.com:3306\"\n",
    "db_name = \"income\"\n",
    "\n",
    "books_df_joined.limit(100).write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"url\", f\"jdbc:mysql://{SERVER_ADDR_port}/{db_name}\") \\\n",
    "  .option(\"dbtable\", \"books_data_mart\") \\\n",
    "  .option(\"user\", \"admin\") \\\n",
    "  .option(\"password\", \"Logindatabase24\") \\\n",
    "  .save()\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4513f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   .option(\"fetchSize\", 50000)\\\n",
    "#   .option(\"isolationLevel\", \"NONE\")\\\n",
    "#   .option(\"queryTimeout\", 600)\\\n",
    " # .option(\"numPartitions\", 10)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8224335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9abcba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44edbb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c45eaed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "704e172b",
   "metadata": {},
   "source": [
    "## handling nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "035233ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d1a3d59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-----+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|         description|             authors|               image|         previewLink|           publisher|       publishedDate|            infoLink|          categories|        ratingsCount|        Id|Price|             User_id|         profileName|  review/helpfulness|        review/score|        review/time|      review/summary|         review/text|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-----+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|\",,['Kensil Bell'...|&dq=%22Always+rea...|&hl=&cd=1&source=...|                    |                1943|http://books.goog...|                    |                    |                    |B0007H2HAM|    0|        stout hearts|  and alert minds\"\"\"|\"It's hard to bel...| it's the narrati...|                   | Bell had already...| 1941) which was ...|\n",
      "|                    |['Allan Scott', '...|                    |http://books.goog...|                    |                1985|http://books.goog...|                    |                    |0804467900|    0|      A15LK8DSFQZZ52|\"Patricia R. Ande...|                 0/0|                 5.0|2003-02-18 08:00:00|Fred and Ginger, ...|This book is the ...|\n",
      "|\",\"Best known for...| John R. Abernath...| Abernathy consid...|             sheriff| Secret Service a...| and wildcat oil ...| he could stun th...| a service for wh...|['John R. Abernat...|B00085T7O2|    0|                 0/1|                 5.0|          1341705600|Fascinating life ...|                   |                    |                    |\n",
      "|\",\"Best known for...| John R. Abernath...| Abernathy consid...|             sheriff| Secret Service a...| and wildcat oil ...| he could stun th...| a service for wh...|['John R. Abernat...|B00085T7O2|    0| terrible forward...|I found the book ...|                    |                    |                   |                    |                    |\n",
      "|                    |['Martha Peterson...|                    |http://books.goog...|                    |          1998-01-01|http://books.goog...|         ['Indiana']|                    |1891598015|    0|        AKNICOEAMLIE|                Dave|                 1/1|                 3.0|2004-03-21 08:00:00|Historical overvi...|I received my cop...|\n",
      "|                    |['Martha Peterson...|                    |http://books.goog...|                    |          1998-01-01|http://books.goog...|         ['Indiana']|                    |1891598015|    0|       AJ197S8I2DAJX|           nauvoo_04|                 0/0|                 3.0|2004-06-24 08:00:00|Review from an In...|I borrowed this b...|\n",
      "|Say I Do to Succe...|                    |http://books.goog...|http://books.goog...|  Entrepreneur Press|          2012-07-15|https://play.goog...|['Business & Econ...|                    |0967086302|    0|                    |                    |                 0/0|                 2.0|2000-04-10 08:00:00|Informative and a...|This book is help...|\n",
      "|Say I Do to Succe...|                    |http://books.goog...|http://books.goog...|  Entrepreneur Press|          2012-07-15|https://play.goog...|['Business & Econ...|                    |0967086302|    0|       A3ZGOSKOHFBY2|  \"\"\"deshawpatton\"\"\"|                 0/2|                 4.0|2003-01-06 08:00:00|        Very Helpful|Thank you for thi...|\n",
      "|Say I Do to Succe...|                    |http://books.goog...|http://books.goog...|  Entrepreneur Press|          2012-07-15|https://play.goog...|['Business & Econ...|                    |0967086302|    0|      A1KUCLWJFY5DFT|      \"\"\"emkemper\"\"\"|               26/28|                 2.0|2000-10-15 08:00:00|frustrating and c...|This book, while ...|\n",
      "|Say I Do to Succe...|                    |http://books.goog...|http://books.goog...|  Entrepreneur Press|          2012-07-15|https://play.goog...|['Business & Econ...|                    |0967086302|    0|                    |                    |               13/13|                 2.0|2001-07-28 08:00:00|Not Enough Info o...|This book may be ...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-----+--------------------+--------------------+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp = books_df_joined.limit(10)\n",
    "\n",
    "replacements = {\n",
    "    \"int\": 0,\n",
    "    \"double\": 0.0,\n",
    "    \"float\": 0.0,\n",
    "    \"string\": \"\"\n",
    "}\n",
    "\n",
    "# Loop through columns and apply replacement rules\n",
    "for col_name, data_type in df_temp.dtypes:\n",
    "    replacement_value = replacements.get(data_type.lower(), None)\n",
    "    if replacement_value is not None:\n",
    "        df_temp = df_temp.withColumn(col_name, when(col(col_name).isNull(), replacement_value).otherwise(col(col_name)))\n",
    "\n",
    "df_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8593964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['description',\n",
       " 'authors',\n",
       " 'image',\n",
       " 'previewLink',\n",
       " 'publisher',\n",
       " 'publishedDate',\n",
       " 'infoLink',\n",
       " 'categories',\n",
       " 'ratingsCount',\n",
       " 'Id',\n",
       " 'Price',\n",
       " 'User_id',\n",
       " 'profileName',\n",
       " 'review/helpfulness',\n",
       " 'review/score',\n",
       " 'review/time',\n",
       " 'review/summary',\n",
       " 'review/text']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df_joined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38b51bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, Word2Vec, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Preprocess the text data\n",
    "preprocessed_df = df_temp.withColumn(\"categories_processed\", lower(col(\"categories\")))\n",
    "\n",
    "# Tokenize the preprocessed text\n",
    "tokenizer = Tokenizer(inputCol=\"categories_processed\", outputCol=\"tokens\")\n",
    "tokenized_df = tokenizer.transform(preprocessed_df)\n",
    "\n",
    "# Vectorize the tokens using Word2Vec\n",
    "word2vec = Word2Vec(vectorSize=100, minCount=5, inputCol=\"tokens\", outputCol=\"features\")\n",
    "model = word2vec.fit(tokenized_df)\n",
    "vectorized_df = model.transform(tokenized_df)\n",
    "\n",
    "# Convert string labels to numeric indices\n",
    "label_indexer = StringIndexer(inputCol=\"categories\", outputCol=\"label\")\n",
    "indexed_df = label_indexer.fit(vectorized_df).transform(vectorized_df)\n",
    "\n",
    "# Train a classification model\n",
    "train_df = indexed_df.filter(indexed_df[\"label\"].isNotNull())\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", predictionCol=\"predicted_label\")\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Impute missing categories\n",
    "predict_df = indexed_df.filter(indexed_df[\"label\"].isNull())\n",
    "imputed_df = model.transform(predict_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c31f087c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "CSV data source does not support array<string> data type.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m imputed_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_nlp.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1240\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1223\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1224\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1239\u001b[0m )\n\u001b[1;32m-> 1240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mcsv(path)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: CSV data source does not support array<string> data type."
     ]
    }
   ],
   "source": [
    "imputed_df.write.csv(\"test_nlp.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91125c0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column categories must be of type numeric but was actually of type string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m lr \u001b[38;5;241m=\u001b[39m LogisticRegression(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_categories\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[lr])\n\u001b[1;32m---> 26\u001b[0m model \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mfit(train_df)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Impute missing categories\u001b[39;00m\n\u001b[0;32m     29\u001b[0m predict_df \u001b[38;5;241m=\u001b[39m vectorized_df\u001b[38;5;241m.\u001b[39mfilter(vectorized_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misNull())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\ml\\pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mfit(dataset)\n\u001b[0;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:379\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 379\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_java(dataset)\n\u001b[0;32m    380\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:376\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: Column categories must be of type numeric but was actually of type string."
     ]
    }
   ],
   "source": [
    "df_temp = books_df_joined.limit(100)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, Word2Vec\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "# Preprocess the text data\n",
    "# Assuming \"categories\" is the column containing text data\n",
    "preprocessed_df = df_temp.withColumn(\"categories_processed\", lower(col(\"categories\")))\n",
    "\n",
    "# Tokenize the preprocessed text\n",
    "tokenizer = Tokenizer(inputCol=\"categories_processed\", outputCol=\"tokens\")\n",
    "tokenized_df = tokenizer.transform(preprocessed_df)\n",
    "\n",
    "# Vectorize the tokens using Word2Vec\n",
    "word2vec = Word2Vec(vectorSize=100, minCount=5, inputCol=\"tokens\", outputCol=\"features\")\n",
    "model = word2vec.fit(tokenized_df)\n",
    "vectorized_df = model.transform(tokenized_df)\n",
    "\n",
    "# Train a classification model\n",
    "# Assuming there are some labels for training\n",
    "train_df = vectorized_df.filter(vectorized_df[\"categories\"].isNotNull())\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"categories\", predictionCol=\"predicted_categories\")\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Impute missing categories\n",
    "predict_df = vectorized_df.filter(vectorized_df[\"categories\"].isNull())\n",
    "imputed_df = model.transform(predict_df)\n",
    "\n",
    "# Select necessary columns and save the DataFrame with imputed categories\n",
    "result_df = imputed_df.select(\"book_id\", \"title\", \"author\", \"categories\", \"predicted_categories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73d8992b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          categories|\n",
      "+--------------------+\n",
      "|                    |\n",
      "|http://books.goog...|\n",
      "|http://books.goog...|\n",
      "|http://books.goog...|\n",
      "|                    |\n",
      "| a service for wh...|\n",
      "| a service for wh...|\n",
      "|http://books.goog...|\n",
      "|http://books.goog...|\n",
      "|http://books.goog...|\n",
      "|http://books.goog...|\n",
      "|http://books.goog...|\n",
      "|http://books.goog...|\n",
      "|http://books.goog...|\n",
      "|http://books.goog...|\n",
      "|http://books.goog...|\n",
      "|         ['Indiana']|\n",
      "|         ['Indiana']|\n",
      "|['Business & Econ...|\n",
      "|['Business & Econ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.select(\"categories\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c16b1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
