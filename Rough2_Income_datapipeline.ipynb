{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8c8cd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\vigne_u98gpxi\\\\Desktop\\\\Lavviggy\\\\Lav\\\\HuntJob\\\\Income\\\\DE_CaseStudy_Dataset\\\\Sales_Data'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520d91da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n",
      "C:\\Users\\vigne_u98gpxi\\anaconda3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(sys.exec_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4481b4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27584484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "674771d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books_data.csv\n",
      "Books_rating.csv\n",
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Price: integer (nullable = true)\n",
      " |-- User_id: string (nullable = true)\n",
      " |-- profileName: string (nullable = true)\n",
      " |-- review/helpfulness: string (nullable = true)\n",
      " |-- review/score: string (nullable = true)\n",
      " |-- review/time: string (nullable = true)\n",
      " |-- review/summary: string (nullable = true)\n",
      " |-- review/text: string (nullable = true)\n",
      "\n",
      "None\n",
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Price: integer (nullable = true)\n",
      " |-- User_id: string (nullable = true)\n",
      " |-- profileName: string (nullable = true)\n",
      " |-- review/helpfulness: string (nullable = true)\n",
      " |-- review/score: string (nullable = true)\n",
      " |-- review/time: string (nullable = true)\n",
      " |-- review/summary: string (nullable = true)\n",
      " |-- review/text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import avg, round, col,sum,count\n",
    "#import org.apache.spark.sql.functions.col\n",
    "#import org.apache.spark.sql.types.IntegerType\n",
    "\n",
    "def create_spark_session(app_name: str) -> SparkSession:\n",
    "    \"\"\" Create a spark session.\n",
    "    \"\"\"\n",
    "#     ss = SparkSession.builder.appName(app_name).getOrCreate()#.master('local').appName(app_name).getOrCreate()\n",
    "#     ss = SparkSession.builder.appName(app_name).config(\"spark.jars\", \"mysql-connector-j-8.3.0.jar\").getOrCreate()#.master('local').appName(app_name).getOrCreate()\n",
    "\n",
    "\n",
    "#     ss = SparkSession.builder.appName(app_name).config(\"spark.jars\", \"spark-mssql-connector_2.12-1.2.0.jar\").getOrCreate()#.master('local').appName(app_name).getOrCreate()\n",
    "#     ss = SparkSession.builder.appName(app_name).config(\"spark.jars\", \"mssql-jdbc-12.6.1.jre11.jar\").getOrCreate()#.master('local').appName(app_name).getOrCreate()    \n",
    "#     ss = SparkSession.builder.appName(app_name).config(\"spark.jars\", \"spark-mssql-connector_2.12-1.4.0-BETA.jar\").getOrCreate()#.master('local').appName(app_name).getOrCreate()\n",
    "\n",
    "#     ss = SparkSession.builder.appName(app_name).config(\"spark.jars\", \"spark-mssql-connector_2.12-1.3.0-BETA.jar\").getOrCreate()#.master('local').appName(app_name).getOrCreate()\n",
    "#     ss = SparkSession.builder.appName(app_name).config(\"spark.jars\", \"spark-mssql-connector_2.12-1.3.0-BETA\").getOrCreate()#.master('local').appName(app_name).getOrCreate()\n",
    "\n",
    "#     ss = SparkSession.builder.appName(app_name).config(\"spark.jars.packages\", \"com.microsoft.azure:spark-mssql-connector_2.12:1.3.0\").getOrCreate()#.master('local').appName(app_name).getOrCreate()\n",
    "\n",
    "    ss = SparkSession.builder.appName(app_name).config(\"spark.jars\", \"spark-mssql-connector_2.12-1.2.0.jar\").getOrCreate()#.master('local').appName(app_name).getOrCreate()\n",
    "\n",
    "#     ss = SparkSession.builder.appName(app_name).config(\"spark.jars.packages\", \"com.microsoft.azure:spark-mssql-connector_2.12:1.2.0\").getOrCreate()#.master('local').appName(app_name).getOrCreate()\n",
    "\n",
    "    \n",
    "\n",
    "    return ss\n",
    "\n",
    "\n",
    "def read_in_data(sc: SparkSession, file: str):\n",
    "    \"\"\" Return a spark DataFrame of the excel file <file>.\n",
    "    \"\"\"\n",
    "    print(file)\n",
    "    return sc.read.csv(file, header='true', sep=',', inferSchema=True)\n",
    "\n",
    "\n",
    "def sum_by_feature(df: DataFrame, feature: str) -> DataFrame:\n",
    "    \"\"\" Find the average salary of <feature> by groups.\n",
    "    \"\"\"\n",
    "   \n",
    "    return df.groupby(feature) \\\n",
    "        .agg(sum('Price').alias('Total_Price')) \\\n",
    "        .sort('Total_Price', ascending=False)\n",
    "\n",
    "def count_by_feature(df: DataFrame, feature: str) -> DataFrame:\n",
    "    \"\"\" Find the average salary of <feature> by groups.\n",
    "    \"\"\"\n",
    "    df.printSchema()\n",
    "    return df.groupby(feature) \\\n",
    "        .agg(count('Price').alias('Total_Quantity')) \\\n",
    "        .sort('Total_Quantity', ascending=False)\n",
    "\n",
    "\n",
    "def output_result(df: DataFrame, output_location: str, output_folder: str) -> None:\n",
    "    \"\"\" Save the DataFrame <df> as a csv file in the location specified by\n",
    "    <output_location>.\n",
    "    \"\"\"\n",
    "    # condense all data points in one single file\n",
    "    df.coalesce(1).write.csv(path=output_location + output_folder,\n",
    "                             mode='append', header=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # create a spark session\n",
    "    spark = create_spark_session(\"average salary\")\n",
    "\n",
    "    #### EXTRACT ####\n",
    "\n",
    "    # read in the train_features.csv as a spark DataFrame\n",
    "    books_df = read_in_data(spark, 'books_data.csv')\n",
    "\n",
    "    # read in the train_salaries.csv as a spark DataFrame\n",
    "    books_ratings_df = read_in_data(spark, 'Books_rating.csv')\n",
    "    \n",
    "\n",
    "    #### TRANSFORM ####\n",
    "\n",
    "\n",
    "    df=books_ratings_df.withColumn('Price',col('Price').cast(\"integer\"))\n",
    "    print(df.printSchema())\n",
    "  \n",
    "    sum_price_by_title = sum_by_feature(df, 'Title')\n",
    "    count_price_by_title = count_by_feature(df, 'Title')\n",
    "    \n",
    "\n",
    "    #### LOAD ####\n",
    "\n",
    "    #OUTPUT_LOCATION = '/output'\n",
    "\n",
    "    # save the above results as csv files in the bucket\n",
    "#     output_result(sum_price_by_title, OUTPUT_LOCATION, 'Title')\n",
    "#     output_result(count_price_by_title, OUTPUT_LOCATION, 'Title')\n",
    "  \n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d450c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# books_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb175b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TABLE books_data (\n",
    "#     Title String,\n",
    "#     description String,\n",
    "#     authors String,\n",
    "#     image String\n",
    "#     previewLink String\n",
    "#     publisher String\n",
    "#     publishedDate String\n",
    "#     infoLink String\n",
    "#     categories String\n",
    "#     ratingsCount String\n",
    "   \n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ec073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# books_df.limit(2).write \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "#     .option(\"url\", \"jdbc:mysql://income-sales-db.cjuyq4k026ji.us-east-2.rds.amazonaws.com:1433/income\") \\\n",
    "#     .option(\"dbtable\", \"sales.books_data\") \\\n",
    "#     .option(\"user\", \"admin\") \\\n",
    "#     .option(\"password\", \"Logindatabase24\") \\\n",
    "#     .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e57c5d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SERVER_ADDR_port = \"income-sales-db.cjuyq4k026ji.us-east-2.rds.amazonaws.com:1433\"\n",
    "# db_name = \"income\"\n",
    "\n",
    "# books_df.limit(2).write \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .option(\"driver\" , \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "#     .option(\"url\", f\"jdbc:sqlserver://{SERVER_ADDR_port};databaseName={db_name};integratedSecurity=true;encrypt=true;trustServerCertificate=true;\") \\\n",
    "#     .option(\"dbtable\", \"sales.books_data\") \\\n",
    "#     .option(\"user\", \"admin\") \\\n",
    "#     .option(\"password\", \"Logindatabase24\") \\\n",
    "#     .save()\n",
    "\n",
    "\n",
    "# #     .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "# #     .option(\"driver\" , \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "\n",
    "\n",
    "# #This driver is not configured for integrated authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d8ac78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "571b6b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SERVER_ADDR_port = \"income-sales-db.cjuyq4k026ji.us-east-2.rds.amazonaws.com:1433\"\n",
    "# # SERVER_ADDR_port = \"income-sales-db.cjuyq4k026ji.us-east-2.rds.amazonaws.com\"\n",
    "\n",
    "# db_name = \"income\"\n",
    "\n",
    "# books_df.limit(2).write \\\n",
    "#     .format(\"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "#     .option(\"url\",f\"jdbc:sqlserver://{SERVER_ADDR_port};databaseName={db_name};\") \\\n",
    "#     .option(\"dbtable\", \"sales.books_data\") \\\n",
    "#     .option(\"user\", \"admin\") \\\n",
    "#     .option(\"password\", \"Logindatabase24\") \\\n",
    "#     .save()\n",
    "\n",
    "#com.microsoft.sqlserver.jdbc.SQLServerDriver does not allow create table as select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ef0553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SERVER_ADDR_port = \"income-sales-db.cjuyq4k026ji.us-east-2.rds.amazonaws.com:1433\"\n",
    "# # SERVER_ADDR_port = \"income-sales-db.cjuyq4k026ji.us-east-2.rds.amazonaws.com\"\n",
    "\n",
    "# db_name = \"income\"\n",
    "\n",
    "# books_df.limit(2).write \\\n",
    "#     .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "#     .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .option(\"url\",f\"jdbc:sqlserver://{SERVER_ADDR_port};databaseName={db_name};\") \\\n",
    "#     .option(\"dbtable\", \"sales.books_data\") \\\n",
    "#     .option(\"user\", \"admin\") \\\n",
    "#     .option(\"password\", \"Logindatabase24\") \\\n",
    "#     .save()\n",
    "\n",
    "# #java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf8a99b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o88.save.\n: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:102)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:102)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:102)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:215)\r\n\tat com.microsoft.sqlserver.jdbc.spark.SQLServerBulkJdbcOptions.<init>(SQLServerBulkJdbcOptions.scala:25)\r\n\tat com.microsoft.sqlserver.jdbc.spark.SQLServerBulkJdbcOptions.<init>(SQLServerBulkJdbcOptions.scala:27)\r\n\tat com.microsoft.sqlserver.jdbc.spark.DefaultSource.createRelation(DefaultSource.scala:55)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m\n\u001b[0;32m      2\u001b[0m SERVER_ADDR_port \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincome-sales-db.cjuyq4k026ji.us-east-2.rds.amazonaws.com\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m db_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincome\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m books_df\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mwrite \\\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcom.microsoft.sqlserver.jdbc.spark\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcom.microsoft.sqlserver.jdbc.SQLServerDriver\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjdbc:sqlserver://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSERVER_ADDR_port\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;databaseName=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales.books_data\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madmin\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogindatabase24\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1107\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1305\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o88.save.\n: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:102)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:102)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:102)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:215)\r\n\tat com.microsoft.sqlserver.jdbc.spark.SQLServerBulkJdbcOptions.<init>(SQLServerBulkJdbcOptions.scala:25)\r\n\tat com.microsoft.sqlserver.jdbc.spark.SQLServerBulkJdbcOptions.<init>(SQLServerBulkJdbcOptions.scala:27)\r\n\tat com.microsoft.sqlserver.jdbc.spark.DefaultSource.createRelation(DefaultSource.scala:55)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "# SERVER_ADDR_port = \"income-sales-db.cjuyq4k026ji.us-east-2.rds.amazonaws.com:1433\"\n",
    "SERVER_ADDR_port = \"income-sales-db.cjuyq4k026ji.us-east-2.rds.amazonaws.com\"\n",
    "db_name = \"income\"\n",
    "\n",
    "books_df.limit(2).write \\\n",
    "    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"url\",f\"jdbc:sqlserver://{SERVER_ADDR_port};databaseName={db_name};\") \\\n",
    "    .option(\"dbtable\", \"sales.books_data\") \\\n",
    "    .option(\"user\", \"admin\") \\\n",
    "    .option(\"password\", \"Logindatabase24\") \\\n",
    "    .save()\n",
    "\n",
    "#java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4513f14e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8224335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9abcba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
